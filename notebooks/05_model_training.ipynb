{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered features input: ../data/processed/engineered_features_2023_sample.parquet\n",
      "Models dir: ../models\n",
      "Output dir: ../output\n",
      "\n",
      "Loaded engineered dataset: (1960167, 54)\n",
      "\n",
      "Train size: (1568133, 54), Test size: (392034, 54)\n",
      "Split indices saved to: ../data/processed/train_test_split_indices.csv\n",
      "Dropping datetime columns from features: ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'pickup_date']\n",
      "\n",
      "Numeric columns: 41\n",
      "Categorical columns: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/lib/_nanfunctions_impl.py:1214: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Initial numeric features: 40\n",
      "Initial categorical features: 10\n",
      "\n",
      "Categorical cardinalities:\n",
      "  store_and_fwd_flag: 2\n",
      "  pickup_borough: 6\n",
      "  pickup_zone: 246\n",
      "  temp_category: 1\n",
      "  dropoff_borough: 6\n",
      "  dropoff_zone: 257\n",
      "  zone_pair_encoded: 16955\n",
      "  traffic_volume_category: 2\n",
      "  time_of_day_factor: 4\n",
      "  day_type: 3\n",
      "\n",
      "Will ONE-HOT encode these (low-card): ['store_and_fwd_flag', 'pickup_borough', 'temp_category', 'dropoff_borough', 'traffic_volume_category', 'time_of_day_factor', 'day_type']\n",
      "Will NOT one-hot these (high-card, excluded from X): ['pickup_zone', 'dropoff_zone', 'zone_pair_encoded']\n",
      "Dropping extremely high-card columns: ['zone_pair_encoded']\n",
      "\n",
      "Final low-card categoricals to one-hot: ['store_and_fwd_flag', 'pickup_borough', 'temp_category', 'dropoff_borough', 'traffic_volume_category', 'time_of_day_factor', 'day_type']\n",
      "High-card categoricals (ignored in model features): ['pickup_zone', 'dropoff_zone']\n",
      "Scaler saved to: ../models/scaler.pkl\n",
      "Encoder saved to: ../models/encoder.pkl\n",
      "\n",
      "X_train shape: (1568133, 57)\n",
      "X_test shape : (392034, 57)\n",
      "y_train shape: (1568133,)\n",
      "y_test shape : (392034,)\n",
      "\n",
      "Feature dtypes:\n",
      "VendorID           float64\n",
      "passenger_count    float64\n",
      "trip_distance      float64\n",
      "RatecodeID         float64\n",
      "PULocationID       float64\n",
      "dtype: object\n",
      "\n",
      "Train feature summary (head):\n",
      "                     count          mean  std       min       25%       50%  \\\n",
      "VendorID         1568133.0  1.571761e-16  1.0 -1.657658 -1.657658  0.603261   \n",
      "passenger_count  1568133.0  1.855954e-17  1.0 -1.511794 -0.420499 -0.420499   \n",
      "trip_distance    1568133.0 -1.681959e-17  1.0 -0.184471 -0.128541 -0.091083   \n",
      "RatecodeID       1568133.0  6.669835e-18  1.0 -0.071962 -0.071962 -0.071962   \n",
      "PULocationID     1568133.0 -5.915854e-17  1.0 -2.579464 -0.531552 -0.062564   \n",
      "\n",
      "                      75%         max  \n",
      "VendorID         0.603261    0.603261  \n",
      "passenger_count -0.420499    7.218564  \n",
      "trip_distance   -0.005393  496.544131  \n",
      "RatecodeID      -0.071962   16.219967  \n",
      "PULocationID     1.063006    1.547627  \n",
      "\n",
      "Test feature summary (head):\n",
      "                    count      mean       std       min       25%       50%  \\\n",
      "VendorID         392034.0 -0.012290  1.006385 -1.657658 -1.657658  0.603261   \n",
      "passenger_count  392034.0 -0.045878  0.963434 -1.511794 -0.420499 -0.420499   \n",
      "trip_distance    392034.0 -0.018651  1.488234 -0.184471 -0.130593 -0.095701   \n",
      "RatecodeID       392034.0 -0.002304  1.003440 -0.071962 -0.071962 -0.071962   \n",
      "PULocationID     392034.0  0.023610  1.002044 -2.579464 -0.531552 -0.062564   \n",
      "\n",
      "                      75%         max  \n",
      "VendorID         0.603261    0.603261  \n",
      "passenger_count -0.420499    7.218564  \n",
      "trip_distance   -0.024891  496.705763  \n",
      "RatecodeID      -0.071962   16.219967  \n",
      "PULocationID     1.063006    1.547627  \n",
      "\n",
      "Saved:\n",
      "X_train -> ../data/processed/X_train.parquet\n",
      "X_test  -> ../data/processed/X_test.parquet\n",
      "y_train -> ../data/processed/y_train.parquet\n",
      "y_test  -> ../data/processed/y_test.parquet\n",
      "\n",
      "Target (fare_amount) stats – train:\n",
      "count    1.568133e+06\n",
      "mean     1.893173e+01\n",
      "std      1.770179e+01\n",
      "min      1.000000e-02\n",
      "25%      8.600000e+00\n",
      "50%      1.280000e+01\n",
      "75%      2.050000e+01\n",
      "max      6.568000e+02\n",
      "Name: fare_amount, dtype: float64\n",
      "\n",
      "Target (fare_amount) stats – test:\n",
      "count    392034.000000\n",
      "mean         17.878323\n",
      "std          15.982030\n",
      "min           0.010000\n",
      "25%           8.600000\n",
      "50%          12.800000\n",
      "75%          19.800000\n",
      "max         600.000000\n",
      "Name: fare_amount, dtype: float64\n",
      "Target histograms saved to: ../output\n",
      "Correlation matrix saved to: ../output/correlation_matrix.csv\n",
      "\n",
      "Highly correlated numeric feature pairs (|corr| > 0.9):\n",
      "trip_distance - distance_squared: corr=0.971\n",
      "trip_distance - weather_distance: corr=0.987\n",
      "temperature_avg - temp_fahrenheit: corr=1.000\n",
      "temperature_avg - temp_deviation: corr=1.000\n",
      "is_raining - extreme_weather: corr=1.000\n",
      "month - quarter: corr=0.997\n",
      "week_of_year - is_holiday: corr=0.998\n",
      "trip_duration_minutes - duration_squared: corr=0.903\n",
      "distance_squared - weather_distance: corr=0.977\n",
      "temp_fahrenheit - temp_deviation: corr=1.000\n",
      "\n",
      "=== FINAL DATA PREP SUMMARY ===\n",
      "Total samples: 1960167\n",
      "Train samples: 1568133\n",
      "Test samples : 392034\n",
      "Train/Test ratio: 0.800 / 0.200\n",
      "Total features after encoding: 57\n",
      "\n",
      "Prepared data locations:\n",
      "X_train: ../data/processed/X_train.parquet\n",
      "X_test : ../data/processed/X_test.parquet\n",
      "y_train: ../data/processed/y_train.parquet\n",
      "y_test : ../data/processed/y_test.parquet\n",
      "Scaler : ../models/scaler.pkl\n",
      "Encoder: ../models/encoder.pkl\n",
      "Correlation matrix: ../output/correlation_matrix.csv\n",
      "\n",
      " Data is ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 0. PATHS \n",
    "# ============================================================\n",
    "# Data base\n",
    "if os.path.isdir(\"/data\"):\n",
    "    DATA_BASE = \"/data\"\n",
    "else:\n",
    "    DATA_BASE = \"../data\"\n",
    "\n",
    "PROCESSED_DIR = os.path.join(DATA_BASE, \"processed\")\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Models base\n",
    "if os.path.isdir(\"/models\"):\n",
    "    MODELS_DIR = \"/models\"\n",
    "else:\n",
    "    MODELS_DIR = \"../models\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Output base\n",
    "if os.path.isdir(\"/output\"):\n",
    "    OUTPUT_DIR = \"/output\"\n",
    "else:\n",
    "    OUTPUT_DIR = \"../output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "engineered_path = os.path.join(PROCESSED_DIR, \"engineered_features_2023.parquet\")\n",
    "if not os.path.exists(engineered_path):\n",
    "    engineered_path = os.path.join(PROCESSED_DIR, \"engineered_features_2023_sample.parquet\")\n",
    "\n",
    "print(\"Engineered features input:\", engineered_path)\n",
    "\n",
    "# Where to save prepared data\n",
    "X_train_path = os.path.join(PROCESSED_DIR, \"X_train.parquet\")\n",
    "X_test_path  = os.path.join(PROCESSED_DIR, \"X_test.parquet\")\n",
    "y_train_path = os.path.join(PROCESSED_DIR, \"y_train.parquet\")\n",
    "y_test_path  = os.path.join(PROCESSED_DIR, \"y_test.parquet\")\n",
    "\n",
    "scaler_path  = os.path.join(MODELS_DIR, \"scaler.pkl\")\n",
    "encoder_path = os.path.join(MODELS_DIR, \"encoder.pkl\")\n",
    "\n",
    "corr_matrix_path   = os.path.join(OUTPUT_DIR, \"correlation_matrix.csv\")\n",
    "split_indices_path = os.path.join(PROCESSED_DIR, \"train_test_split_indices.csv\")\n",
    "\n",
    "print(\"Models dir:\", MODELS_DIR)\n",
    "print(\"Output dir:\", OUTPUT_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD ENGINEERED DATA\n",
    "# ============================================================\n",
    "df = pd.read_parquet(engineered_path)\n",
    "print(\"\\nLoaded engineered dataset:\", df.shape)\n",
    "\n",
    "if \"pickup_date\" not in df.columns:\n",
    "    raise KeyError(\"pickup_date not found in engineered features; needed for time-based split.\")\n",
    "\n",
    "df[\"pickup_date\"] = pd.to_datetime(df[\"pickup_date\"])\n",
    "\n",
    "# ============================================================\n",
    "# 2. TIME-BASED TRAIN / TEST SPLIT (80/20)\n",
    "# ============================================================\n",
    "df = df.sort_values(\"pickup_date\").reset_index(drop=True)\n",
    "\n",
    "n_samples = len(df)\n",
    "split_idx = int(0.8 * n_samples)\n",
    "\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df  = df.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"\\nTrain size: {train_df.shape}, Test size: {test_df.shape}\")\n",
    "\n",
    "# Save split indices (for reproducibility / reference)\n",
    "split_indices = pd.DataFrame({\n",
    "    \"index\": np.arange(n_samples),\n",
    "    \"set\": [\"train\"] * split_idx + [\"test\"] * (n_samples - split_idx)\n",
    "})\n",
    "split_indices.to_csv(split_indices_path, index=False)\n",
    "print(\"Split indices saved to:\", split_indices_path)\n",
    "\n",
    "# Drop datetime columns from features \n",
    "datetime_cols = train_df.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns.tolist()\n",
    "print(\"Dropping datetime columns from features:\", datetime_cols)\n",
    "\n",
    "train_df = train_df.drop(columns=datetime_cols)\n",
    "test_df  = test_df.drop(columns=datetime_cols)\n",
    "\n",
    "# ============================================================\n",
    "# 3. HANDLE MISSING VALUES \n",
    "# ============================================================\n",
    "target_col = \"fare_amount\"\n",
    "if target_col not in df.columns:\n",
    "    raise KeyError(\"Target column 'fare_amount' not found in engineered dataset.\")\n",
    "\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric columns:\", len(numeric_cols))\n",
    "print(\"Categorical columns:\", len(categorical_cols))\n",
    "\n",
    "for col in numeric_cols:\n",
    "    # Compute median using TRAINING DATA ONLY to avoid leakage.\n",
    "    median_train = train_df[col].median()\n",
    "\n",
    "    # If the median is NaN (e.g., all values are missing), fall back to a safe constant.\n",
    "    if pd.isna(median_train):\n",
    "        median_val = 0.0\n",
    "    else:\n",
    "        median_val = median_train\n",
    "\n",
    "    train_df[col] = train_df[col].fillna(median_val)\n",
    "    test_df[col]  = test_df[col].fillna(median_val)\n",
    "\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Compute mode using TRAINING DATA ONLY to avoid leakage.\n",
    "    mode_train = train_df[col].mode(dropna=True)\n",
    "    if len(mode_train) > 0:\n",
    "        mode_val = mode_train.iloc[0]\n",
    "    else:\n",
    "        # If even training data has no valid category, fall back to a generic \"missing\" label.\n",
    "        mode_val = \"missing\"\n",
    "\n",
    "    train_df[col] = train_df[col].fillna(mode_val)\n",
    "    test_df[col]  = test_df[col].fillna(mode_val)\n",
    "\n",
    "\n",
    "remaining_train_nans = train_df[numeric_cols + categorical_cols].isna().sum()\n",
    "remaining_test_nans  = test_df[numeric_cols + categorical_cols].isna().sum()\n",
    "\n",
    "if remaining_train_nans.sum() != 0 or remaining_test_nans.sum() != 0:\n",
    "    print(\"\\n Still found NaNs after imputation:\")\n",
    "    print(\"Train NaNs:\\n\", remaining_train_nans[remaining_train_nans > 0])\n",
    "    print(\"Test NaNs:\\n\", remaining_test_nans[remaining_test_nans > 0])\n",
    "    raise AssertionError(\"Train/test still have NaNs after robust imputation!\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. TARGET VARIABLE & REMOVE FROM FEATURES\n",
    "# ============================================================\n",
    "y_train = train_df[target_col].copy()\n",
    "y_test  = test_df[target_col].copy()\n",
    "\n",
    "train_df = train_df.drop(columns=[target_col])\n",
    "test_df  = test_df.drop(columns=[target_col])\n",
    "\n",
    "numeric_features = [c for c in numeric_cols if c != target_col]\n",
    "categorical_features = categorical_cols[:]  \n",
    "\n",
    "print(\"\\n Initial numeric features:\", len(numeric_features))\n",
    "print(\"Initial categorical features:\", len(categorical_features))\n",
    "\n",
    "# ============================================================\n",
    "# 4b. LIMIT ONE-HOT ENCODING TO LOW-CARDINALITY CATEGORICALS\n",
    "# ============================================================\n",
    "print(\"\\nCategorical cardinalities:\")\n",
    "cardinalities = {}\n",
    "for c in categorical_features:\n",
    "    nuniq = train_df[c].nunique()\n",
    "    cardinalities[c] = nuniq\n",
    "    print(f\"  {c}: {nuniq}\")\n",
    "\n",
    "LOW_CARD_THRESHOLD = 50  \n",
    "\n",
    "categorical_low_card = [\n",
    "    c for c in categorical_features\n",
    "    if cardinalities[c] <= LOW_CARD_THRESHOLD\n",
    "]\n",
    "\n",
    "categorical_high_card = [\n",
    "    c for c in categorical_features\n",
    "    if c not in categorical_low_card\n",
    "]\n",
    "\n",
    "print(\"\\nWill ONE-HOT encode these (low-card):\", categorical_low_card)\n",
    "print(\"Will NOT one-hot these (high-card, excluded from X):\", categorical_high_card)\n",
    "\n",
    "cols_to_drop_completely = [\"zone_pair_encoded\"]  \n",
    "drop_now = [c for c in cols_to_drop_completely if c in train_df.columns]\n",
    "\n",
    "if drop_now:\n",
    "    print(\"Dropping extremely high-card columns:\", drop_now)\n",
    "    train_df = train_df.drop(columns=drop_now)\n",
    "    test_df  = test_df.drop(columns=drop_now)\n",
    "    categorical_low_card = [c for c in categorical_low_card if c not in drop_now]\n",
    "    categorical_high_card = [c for c in categorical_high_card if c not in drop_now]\n",
    "\n",
    "print(\"\\nFinal low-card categoricals to one-hot:\", categorical_low_card)\n",
    "print(\"High-card categoricals (ignored in model features):\", categorical_high_card)\n",
    "\n",
    "# ============================================================\n",
    "# 5. FEATURE SCALING (StandardScaler on numeric features)\n",
    "# ============================================================\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[numeric_features])\n",
    "\n",
    "X_train_num = scaler.transform(train_df[numeric_features])\n",
    "X_test_num  = scaler.transform(test_df[numeric_features])\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(\"Scaler saved to:\", scaler_path)\n",
    "\n",
    "# ============================================================\n",
    "# 6. CATEGORICAL ENCODING (OneHotEncoder, drop_first=True)\n",
    "#    NOTE: sklearn>=1.2 uses sparse_output instead of sparse\n",
    "# ============================================================\n",
    "encoder = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    drop=\"first\",\n",
    "    sparse_output=False \n",
    ")\n",
    "\n",
    "if categorical_low_card:\n",
    "    encoder.fit(train_df[categorical_low_card])\n",
    "    X_train_cat = encoder.transform(train_df[categorical_low_card])\n",
    "    X_test_cat  = encoder.transform(test_df[categorical_low_card])\n",
    "    encoded_cat_feature_names = encoder.get_feature_names_out(categorical_low_card)\n",
    "else:\n",
    "    X_train_cat = np.empty((len(train_df), 0))\n",
    "    X_test_cat  = np.empty((len(test_df), 0))\n",
    "    encoded_cat_feature_names = np.array([])\n",
    "\n",
    "# Save encoder\n",
    "joblib.dump(encoder, encoder_path)\n",
    "print(\"Encoder saved to:\", encoder_path)\n",
    "\n",
    "# ============================================================\n",
    "# 7. COMBINE NUMERIC + CATEGORICAL INTO FINAL MATRICES\n",
    "# ============================================================\n",
    "X_train = np.hstack([X_train_num, X_train_cat])\n",
    "X_test  = np.hstack([X_test_num, X_test_cat])\n",
    "\n",
    "feature_names = numeric_features + encoded_cat_feature_names.tolist()\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "X_test_df  = pd.DataFrame(X_test,  columns=feature_names)\n",
    "\n",
    "# ============================================================\n",
    "# 8. VALIDATION: SHAPES, NANs, INFs\n",
    "# ============================================================\n",
    "print(\"\\nX_train shape:\", X_train_df.shape)\n",
    "print(\"X_test shape :\", X_test_df.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape :\", y_test.shape)\n",
    "\n",
    "assert np.isfinite(X_train_df.to_numpy()).all(), \"X_train contains NaN or inf!\"\n",
    "assert np.isfinite(X_test_df.to_numpy()).all(), \"X_test contains NaN or inf!\"\n",
    "\n",
    "print(\"\\nFeature dtypes:\")\n",
    "print(X_train_df.dtypes.head())\n",
    "print(\"\\nTrain feature summary (head):\")\n",
    "print(X_train_df.describe().T.head())\n",
    "print(\"\\nTest feature summary (head):\")\n",
    "print(X_test_df.describe().T.head())\n",
    "\n",
    "# ============================================================\n",
    "# 9. SAVE PREPARED DATASETS\n",
    "# ============================================================\n",
    "X_train_df.to_parquet(X_train_path, index=False)\n",
    "X_test_df.to_parquet(X_test_path, index=False)\n",
    "y_train.to_frame(name=target_col).to_parquet(y_train_path, index=False)\n",
    "y_test.to_frame(name=target_col).to_parquet(y_test_path, index=False)  \n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"X_train ->\", X_train_path)\n",
    "print(\"X_test  ->\", X_test_path)\n",
    "print(\"y_train ->\", y_train_path)\n",
    "print(\"y_test  ->\", y_test_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. TARGET DISTRIBUTION & HISTOGRAMS\n",
    "# ============================================================\n",
    "print(\"\\nTarget (fare_amount) stats – train:\")\n",
    "print(y_train.describe())\n",
    "print(\"\\nTarget (fare_amount) stats – test:\")\n",
    "print(y_test.describe())\n",
    "\n",
    "# Histograms of target\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y_train, bins=80, alpha=0.7)\n",
    "plt.title(\"Fare Amount – Train\")\n",
    "plt.xlabel(\"fare_amount\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"target_hist_train.png\"))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y_test, bins=80, alpha=0.7)\n",
    "plt.title(\"Fare Amount – Test\")\n",
    "plt.xlabel(\"fare_amount\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"target_hist_test.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"Target histograms saved to:\", OUTPUT_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 11. CORRELATION ANALYSIS (on numeric features, train only)\n",
    "# ============================================================\n",
    "train_numeric_only = pd.DataFrame(\n",
    "    X_train_num, columns=numeric_features\n",
    ")\n",
    "corr_matrix = train_numeric_only.corr()\n",
    "corr_matrix.to_csv(corr_matrix_path)\n",
    "print(\"Correlation matrix saved to:\", corr_matrix_path)\n",
    "\n",
    "high_corr_pairs = []\n",
    "threshold = 0.9\n",
    "for i in range(len(numeric_features)):\n",
    "    for j in range(i+1, len(numeric_features)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > threshold:\n",
    "            high_corr_pairs.append(\n",
    "                (numeric_features[i], numeric_features[j], corr_val)\n",
    "            )\n",
    "\n",
    "print(\"\\nHighly correlated numeric feature pairs (|corr| > 0.9):\")\n",
    "for a, b, c in high_corr_pairs:\n",
    "    print(f\"{a} - {b}: corr={c:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 12. FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n=== FINAL DATA PREP SUMMARY ===\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Train samples: {len(y_train)}\")\n",
    "print(f\"Test samples : {len(y_test)}\")\n",
    "print(f\"Train/Test ratio: {len(y_train) / len(df):.3f} / {len(y_test) / len(df):.3f}\")\n",
    "print(f\"Total features after encoding: {X_train_df.shape[1]}\")\n",
    "\n",
    "print(\"\\nPrepared data locations:\")\n",
    "print(\"X_train:\", X_train_path)\n",
    "print(\"X_test :\", X_test_path)\n",
    "print(\"y_train:\", y_train_path)\n",
    "print(\"y_test :\", y_test_path)\n",
    "print(\"Scaler :\", scaler_path)\n",
    "print(\"Encoder:\", encoder_path)\n",
    "print(\"Correlation matrix:\", corr_matrix_path)\n",
    "\n",
    "print(\"\\n Data is ready for modeling.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
