{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0. IMPORTS\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPARISON: LR vs RF vs HistGradientBoosting ===\n",
      "DATA_BASE   : ../data\n",
      "PROCESSED   : ../data/processed\n",
      "MODELS_DIR  : ../models\n",
      "OUTPUT_DIR  : ../output\n",
      "VIS_DIR     : ../output/model_comparison\n",
      "\n",
      "Loading prepared data...\n",
      "X_train: (1568133, 57) X_test: (392034, 57)\n",
      "y_train: (1568133,) y_test: (392034,)\n",
      "\n",
      "Loading trained models from: ../models\n",
      "✔ Loaded Linear Regression\n",
      "✔ Loaded Random Forest\n",
      "✔ Loaded HistGradientBoostingRegressor\n",
      "\n",
      "Evaluating models...\n",
      "\n",
      "--- Linear Regression ---\n",
      "\n",
      "--- Random Forest ---\n",
      "\n",
      "--- HistGradientBoosting ---\n",
      "\n",
      "Model comparison table saved to: ../output/model_comparison.csv\n",
      "                      Train_RMSE  Test_RMSE  Train_MAE  Test_MAE  Train_MAPE  \\\n",
      "Model                                                                          \n",
      "Linear Regression       5.307594  12.508668   2.116691  2.196437   21.452854   \n",
      "Random Forest           1.656948   2.116527   0.403223  0.559040   12.437571   \n",
      "HistGradientBoosting    2.230088   2.318082   0.623631  0.636380   16.260962   \n",
      "\n",
      "                      Test_MAPE  Train_R2   Test_R2  Train_Time_s  Pred_Time_s  \n",
      "Model                                                                           \n",
      "Linear Regression     18.627779  0.910100  0.387425           NaN     0.253020  \n",
      "Random Forest         11.529472  0.991238  0.982462           NaN     9.622514  \n",
      "HistGradientBoosting  12.734553  0.984129  0.978962           NaN     2.112024  \n",
      "Saved: ../output/model_comparison/rmse_comparison.png\n",
      "Saved: ../output/model_comparison/mae_comparison.png\n",
      "Saved: ../output/model_comparison/r2_comparison.png\n",
      "Saved: ../output/model_comparison/training_time_comparison.png\n",
      "\n",
      "Average absolute error by fare range:\n",
      "                  Model       FareRange  MeanAbsError\n",
      "0     Linear Regression      Low (0–15]      1.422980\n",
      "1     Linear Regression  Medium (15–50]      2.656449\n",
      "2     Linear Regression      High (50+)      8.162218\n",
      "3         Random Forest      Low (0–15]      0.355911\n",
      "4         Random Forest  Medium (15–50]      0.754306\n",
      "5         Random Forest      High (50+)      1.685341\n",
      "6  HistGradientBoosting      Low (0–15]      0.384247\n",
      "7  HistGradientBoosting  Medium (15–50]      0.839535\n",
      "8  HistGradientBoosting      High (50+)      2.266405\n",
      "Fare range error table saved to: ../output/fare_range_errors.csv\n",
      "Saved: ../output/model_comparison/residual_histograms.png\n",
      "Saved: ../output/model_comparison/residuals_vs_predicted.png\n",
      "\n",
      "Prediction correlations between models:\n",
      "                      Linear Regression  Random Forest  HistGradientBoosting\n",
      "Linear Regression              1.000000       0.757743              0.757935\n",
      "Random Forest                  0.757743       1.000000              0.997113\n",
      "HistGradientBoosting           0.757935       0.997113              1.000000\n",
      "Prediction correlation matrix saved to: ../output/prediction_correlations.csv\n",
      "\n",
      "Feature importance comparison saved to: ../output/feature_importance_comparison.csv\n",
      "Saved: ../output/model_comparison/top20_features_avg_importance.png\n",
      "\n",
      "=== BEST MODEL SELECTION ===\n",
      "Best model (lowest Test RMSE): Random Forest\n",
      "Test RMSE: 2.116527021118823\n",
      "Train RMSE: 1.656947692054232\n",
      "RMSE Gap (Test - Train): 0.4595793290645911\n",
      "\n",
      "Model with largest Train-Test RMSE gap (most overfitting): Linear Regression\n",
      "\n",
      "Summary report saved to: ../output/model_selection_report.txt\n",
      "\n",
      " Model comparison & selection completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/mhx6w8j52_d7vbz5h9bq_djc0000gn/T/ipykernel_22159/1742821039.py:352: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.assign(NormImp=g[\"Importance\"] / g[\"Importance\"].sum()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"=== MODEL COMPARISON: LR vs RF vs HistGradientBoosting ===\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. PATH HANDLING \n",
    "# ============================================================\n",
    "if os.path.isdir(\"/data\"):\n",
    "    DATA_BASE = \"/data\"\n",
    "else:\n",
    "    DATA_BASE = \"../data\"\n",
    "\n",
    "if os.path.isdir(\"/models\"):\n",
    "    MODELS_DIR = \"/models\"\n",
    "else:\n",
    "    MODELS_DIR = \"../models\"\n",
    "\n",
    "if os.path.isdir(\"/output\"):\n",
    "    OUTPUT_DIR = \"/output\"\n",
    "else:\n",
    "    OUTPUT_DIR = \"../output\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "VIS_DIR = os.path.join(OUTPUT_DIR, \"model_comparison\")\n",
    "os.makedirs(VIS_DIR, exist_ok=True)\n",
    "\n",
    "PROCESSED_DIR = os.path.join(DATA_BASE, \"processed\")\n",
    "\n",
    "print(\"DATA_BASE   :\", DATA_BASE)\n",
    "print(\"PROCESSED   :\", PROCESSED_DIR)\n",
    "print(\"MODELS_DIR  :\", MODELS_DIR)\n",
    "print(\"OUTPUT_DIR  :\", OUTPUT_DIR)\n",
    "print(\"VIS_DIR     :\", VIS_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 2. LOAD DATA (X_train, X_test, y_train, y_test)\n",
    "# ============================================================\n",
    "X_train_path = os.path.join(PROCESSED_DIR, \"X_train.parquet\")\n",
    "X_test_path  = os.path.join(PROCESSED_DIR, \"X_test.parquet\")\n",
    "y_train_path = os.path.join(PROCESSED_DIR, \"y_train.parquet\")\n",
    "y_test_path  = os.path.join(PROCESSED_DIR, \"y_test.parquet\")\n",
    "\n",
    "print(\"\\nLoading prepared data...\")\n",
    "X_train = pd.read_parquet(X_train_path)\n",
    "X_test  = pd.read_parquet(X_test_path)\n",
    "\n",
    "y_train_df = pd.read_parquet(y_train_path)\n",
    "y_test_df  = pd.read_parquet(y_test_path)\n",
    "\n",
    "if y_train_df.shape[1] == 1:\n",
    "    y_train = y_train_df.iloc[:, 0]\n",
    "else:\n",
    "    raise ValueError(\"y_train has more than 1 column; please adjust.\")\n",
    "\n",
    "if y_test_df.shape[1] == 1:\n",
    "    y_test = y_test_df.iloc[:, 0]\n",
    "else:\n",
    "    raise ValueError(\"y_test has more than 1 column; please adjust.\")\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# ============================================================\n",
    "# 3. LOAD TRAINED MODELS\n",
    "# ============================================================\n",
    "print(\"\\nLoading trained models from:\", MODELS_DIR)\n",
    "\n",
    "lr_path   = os.path.join(MODELS_DIR, \"linear_regression_model.pkl\")\n",
    "rf_path   = os.path.join(MODELS_DIR, \"random_forest_model.pkl\")\n",
    "hgbm_path = os.path.join(MODELS_DIR, \"hist_gbm_model.pkl\")  \n",
    "\n",
    "models = {}\n",
    "\n",
    "if os.path.exists(lr_path):\n",
    "    models[\"Linear Regression\"] = joblib.load(lr_path)\n",
    "    print(\"✔ Loaded Linear Regression\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Linear Regression model not found at {lr_path}\")\n",
    "\n",
    "if os.path.exists(rf_path):\n",
    "    models[\"Random Forest\"] = joblib.load(rf_path)\n",
    "    print(\"✔ Loaded Random Forest\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Random Forest model not found at {rf_path}\")\n",
    "\n",
    "if os.path.exists(hgbm_path):\n",
    "    models[\"HistGradientBoosting\"] = joblib.load(hgbm_path)\n",
    "    print(\"✔ Loaded HistGradientBoostingRegressor\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"HistGradientBoosting model not found at {hgbm_path}\\n\"\n",
    "        \"Train it first (PROMPT_10 replacement).\"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# 4. METRIC HELPERS\n",
    "# ============================================================\n",
    "def mape(y_true, y_pred, eps=1e-6):\n",
    "    \"\"\"Mean Absolute Percentage Error in % (ignore near-zero actuals).\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = np.abs(y_true) > eps\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0\n",
    "\n",
    "def compute_metrics(y_true_train, y_pred_train,\n",
    "                    y_true_test, y_pred_test):\n",
    "    \"\"\"Return metrics dict for train & test.\"\"\"\n",
    "    # Train\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    mae_train  = mean_absolute_error(y_true_train, y_pred_train)\n",
    "    mape_train = mape(y_true_train, y_pred_train)\n",
    "    r2_train   = r2_score(y_true_train, y_pred_train)\n",
    "\n",
    "    # Test\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
    "    mae_test  = mean_absolute_error(y_true_test, y_pred_test)\n",
    "    mape_test = mape(y_true_test, y_pred_test)\n",
    "    r2_test   = r2_score(y_true_test, y_pred_test)\n",
    "\n",
    "    return {\n",
    "        \"Train_RMSE\": rmse_train,\n",
    "        \"Test_RMSE\": rmse_test,\n",
    "        \"Train_MAE\": mae_train,\n",
    "        \"Test_MAE\": mae_test,\n",
    "        \"Train_MAPE\": mape_train,\n",
    "        \"Test_MAPE\": mape_test,\n",
    "        \"Train_R2\": r2_train,\n",
    "        \"Test_R2\": r2_test,\n",
    "    }\n",
    "\n",
    "def load_training_time(model_key):\n",
    "    \"\"\"\n",
    "    Try to load training time from JSON in MODELS_DIR.\n",
    "    Expected filenames:\n",
    "      - linear_regression_time.json\n",
    "      - random_forest_time.json\n",
    "      - hist_gbm_time.json\n",
    "    \"\"\"\n",
    "    fname_map = {\n",
    "        \"Linear Regression\": \"linear_regression_time.json\",\n",
    "        \"Random Forest\": \"random_forest_time.json\",\n",
    "        \"HistGradientBoosting\": \"hist_gbm_time.json\",\n",
    "    }\n",
    "    fname = fname_map.get(model_key)\n",
    "    if fname is None:\n",
    "        return np.nan\n",
    "    path = os.path.join(MODELS_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            return float(data.get(\"training_time_seconds\", np.nan))\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "# ============================================================\n",
    "# 5. EVALUATE ALL MODELS\n",
    "# ============================================================\n",
    "results = []\n",
    "predictions = {}   \n",
    "residuals  = {}    \n",
    "\n",
    "print(\"\\nEvaluating models...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "\n",
    "    train_time = load_training_time(name)\n",
    "\n",
    "    t0 = time.time()\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test  = model.predict(X_test)\n",
    "    pred_time = time.time() - t0\n",
    "\n",
    "    predictions[name] = y_pred_test\n",
    "    residuals[name]   = y_test.values - y_pred_test\n",
    "\n",
    "    # Metrics\n",
    "    metrics = compute_metrics(y_train, y_pred_train, y_test, y_pred_test)\n",
    "    metrics[\"Model\"] = name\n",
    "    metrics[\"Train_Time_s\"] = train_time\n",
    "    metrics[\"Pred_Time_s\"]  = pred_time\n",
    "\n",
    "    results.append(metrics)\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index(\"Model\")\n",
    "comparison_csv_path = os.path.join(OUTPUT_DIR, \"model_comparison.csv\")\n",
    "results_df.to_csv(comparison_csv_path)\n",
    "print(\"\\nModel comparison table saved to:\", comparison_csv_path)\n",
    "print(results_df)\n",
    "\n",
    "# ============================================================\n",
    "# 6. VISUAL COMPARISON – METRICS\n",
    "# ============================================================\n",
    "def bar_compare(metric_name, ylabel, filename):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = results_df[[f\"Train_{metric_name}\", f\"Test_{metric_name}\"]].plot(\n",
    "        kind=\"bar\",\n",
    "        figsize=(8, 5)\n",
    "    )\n",
    "    plt.title(f\"{metric_name} comparison (Train vs Test)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(VIS_DIR, filename)\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "bar_compare(\"RMSE\", \"RMSE\", \"rmse_comparison.png\")\n",
    "bar_compare(\"MAE\",  \"MAE\",  \"mae_comparison.png\")\n",
    "bar_compare(\"R2\",   \"R² score\", \"r2_comparison.png\")\n",
    "\n",
    "# Training time comparison\n",
    "plt.figure(figsize=(6, 4))\n",
    "results_df[\"Train_Time_s\"].plot(kind=\"bar\")\n",
    "plt.ylabel(\"Training time (s)\")\n",
    "plt.title(\"Training Time Comparison\")\n",
    "plt.xticks(rotation=20, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "tt_path = os.path.join(VIS_DIR, \"training_time_comparison.png\")\n",
    "plt.savefig(tt_path)\n",
    "plt.close()\n",
    "print(\"Saved:\", tt_path)\n",
    "\n",
    "# ============================================================\n",
    "# 7. STATISTICAL ANALYSIS BY FARE RANGE\n",
    "# ============================================================\n",
    "print(\"\\nAverage absolute error by fare range:\")\n",
    "fare_ranges = {\n",
    "    \"Low (0–15]\":   (0, 15),\n",
    "    \"Medium (15–50]\": (15, 50),\n",
    "    \"High (50+)\":   (50, np.inf),\n",
    "}\n",
    "\n",
    "fare_range_rows = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    for range_name, (low, high) in fare_ranges.items():\n",
    "        mask = (y_test > low) & (y_test <= high)\n",
    "        if mask.sum() == 0:\n",
    "            mean_abs_err = np.nan\n",
    "        else:\n",
    "            mean_abs_err = np.mean(\n",
    "                np.abs(y_test[mask].values - y_pred[mask])\n",
    "            )\n",
    "        fare_range_rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"FareRange\": range_name,\n",
    "            \"MeanAbsError\": mean_abs_err\n",
    "        })\n",
    "\n",
    "fare_range_df = pd.DataFrame(fare_range_rows)\n",
    "fare_range_path = os.path.join(OUTPUT_DIR, \"fare_range_errors.csv\")\n",
    "fare_range_df.to_csv(fare_range_path, index=False)\n",
    "print(fare_range_df)\n",
    "print(\"Fare range error table saved to:\", fare_range_path)\n",
    "\n",
    "# ============================================================\n",
    "# 8. RESIDUAL COMPARISON\n",
    "# ============================================================\n",
    "# Residual histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (name, res) in enumerate(residuals.items()):\n",
    "    plt.hist(res, bins=80, alpha=0.4, label=name)\n",
    "plt.xlabel(\"Residual (y_true - y_pred)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residual Distribution Comparison\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "res_hist_path = os.path.join(VIS_DIR, \"residual_histograms.png\")\n",
    "plt.savefig(res_hist_path)\n",
    "plt.close()\n",
    "print(\"Saved:\", res_hist_path)\n",
    "\n",
    "# Residual vs predicted plots (separate figure with subplots)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (name, y_pred) in enumerate(predictions.items(), start=1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(y_pred, residuals[name], s=2, alpha=0.3)\n",
    "    plt.axhline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "    plt.xlabel(\"Predicted fare\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.title(name)\n",
    "plt.tight_layout()\n",
    "res_vs_pred_path = os.path.join(VIS_DIR, \"residuals_vs_predicted.png\")\n",
    "plt.savefig(res_vs_pred_path)\n",
    "plt.close()\n",
    "print(\"Saved:\", res_vs_pred_path)\n",
    "\n",
    "# ============================================================\n",
    "# 9. PREDICTION AGREEMENT (CORRELATION BETWEEN MODELS)\n",
    "# ============================================================\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "corr_pred = pred_df.corr()\n",
    "corr_pred_path = os.path.join(OUTPUT_DIR, \"prediction_correlations.csv\")\n",
    "corr_pred.to_csv(corr_pred_path)\n",
    "print(\"\\nPrediction correlations between models:\")\n",
    "print(corr_pred)\n",
    "print(\"Prediction correlation matrix saved to:\", corr_pred_path)\n",
    "\n",
    "# ============================================================\n",
    "# 10. FEATURE IMPORTANCE COMPARISON\n",
    "# ============================================================\n",
    "fi_rows = []\n",
    "\n",
    "# Linear Regression: absolute coefficients\n",
    "lr_model = models[\"Linear Regression\"]\n",
    "if hasattr(lr_model, \"coef_\"):\n",
    "    lr_coefs = np.abs(lr_model.coef_)\n",
    "    for feat, val in zip(feature_names, lr_coefs):\n",
    "        fi_rows.append({\n",
    "            \"Feature\": feat,\n",
    "            \"Model\": \"Linear Regression\",\n",
    "            \"Importance\": val\n",
    "        })\n",
    "\n",
    "# Random Forest: feature_importances_\n",
    "rf_model = models[\"Random Forest\"]\n",
    "if hasattr(rf_model, \"feature_importances_\"):\n",
    "    rf_imp = rf_model.feature_importances_\n",
    "    for feat, val in zip(feature_names, rf_imp):\n",
    "        fi_rows.append({\n",
    "            \"Feature\": feat,\n",
    "            \"Model\": \"Random Forest\",\n",
    "            \"Importance\": val\n",
    "        })\n",
    "\n",
    "# HistGradientBoosting: feature_importances_\n",
    "hgbm_model = models[\"HistGradientBoosting\"]\n",
    "if hasattr(hgbm_model, \"feature_importances_\"):\n",
    "    hgbm_imp = hgbm_model.feature_importances_\n",
    "    for feat, val in zip(feature_names, hgbm_imp):\n",
    "        fi_rows.append({\n",
    "            \"Feature\": feat,\n",
    "            \"Model\": \"HistGradientBoosting\",\n",
    "            \"Importance\": val\n",
    "        })\n",
    "\n",
    "fi_df = pd.DataFrame(fi_rows)\n",
    "fi_path = os.path.join(OUTPUT_DIR, \"feature_importance_comparison.csv\")\n",
    "fi_df.to_csv(fi_path, index=False)\n",
    "print(\"\\nFeature importance comparison saved to:\", fi_path)\n",
    "\n",
    "avg_imp_df = (\n",
    "    fi_df\n",
    "    .groupby([\"Model\"])\n",
    "    .apply(lambda g: g.assign(NormImp=g[\"Importance\"] / g[\"Importance\"].sum()))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "avg_imp = (\n",
    "    avg_imp_df.groupby(\"Feature\")[\"NormImp\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "top20_features = avg_imp.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top20_features[::-1].plot(kind=\"barh\")  \n",
    "plt.xlabel(\"Average normalized importance (across models)\")\n",
    "plt.title(\"Top 20 Features (Average Importance)\")\n",
    "plt.tight_layout()\n",
    "fi_top_path = os.path.join(VIS_DIR, \"top20_features_avg_importance.png\")\n",
    "plt.savefig(fi_top_path)\n",
    "plt.close()\n",
    "print(\"Saved:\", fi_top_path)\n",
    "\n",
    "# ============================================================\n",
    "# 11. OVERFITTING ANALYSIS & BEST MODEL SELECTION\n",
    "# ============================================================\n",
    "results_df[\"RMSE_Gap\"] = results_df[\"Test_RMSE\"] - results_df[\"Train_RMSE\"]\n",
    "\n",
    "best_idx = results_df[\"Test_RMSE\"].idxmin()\n",
    "best_row = results_df.loc[best_idx]\n",
    "\n",
    "best_model_name = best_idx\n",
    "best_test_rmse = best_row[\"Test_RMSE\"]\n",
    "best_train_rmse = best_row[\"Train_RMSE\"]\n",
    "best_gap = best_row[\"RMSE_Gap\"]\n",
    "\n",
    "print(\"\\n=== BEST MODEL SELECTION ===\")\n",
    "print(\"Best model (lowest Test RMSE):\", best_model_name)\n",
    "print(\"Test RMSE:\", best_test_rmse)\n",
    "print(\"Train RMSE:\", best_train_rmse)\n",
    "print(\"RMSE Gap (Test - Train):\", best_gap)\n",
    "\n",
    "worst_gap_idx = results_df[\"RMSE_Gap\"].idxmax()\n",
    "print(\"\\nModel with largest Train-Test RMSE gap (most overfitting):\", worst_gap_idx)\n",
    "\n",
    "# ============================================================\n",
    "# 12. SUMMARY REPORT\n",
    "# ============================================================\n",
    "report_path = os.path.join(OUTPUT_DIR, \"model_selection_report.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"MODEL COMPARISON & SELECTION REPORT\\n\")\n",
    "    f.write(\"===================================\\n\\n\")\n",
    "    f.write(\"Comparison table (metrics):\\n\")\n",
    "    f.write(results_df.to_string())\n",
    "    f.write(\"\\n\\nOverfitting analysis (RMSE Gap = Test_RMSE - Train_RMSE):\\n\")\n",
    "    f.write(results_df[\"RMSE_Gap\"].to_string())\n",
    "    f.write(\"\\n\\nBest model:\\n\")\n",
    "    f.write(f\"- Name       : {best_model_name}\\n\")\n",
    "    f.write(f\"- Test RMSE  : {best_test_rmse:.4f}\\n\")\n",
    "    f.write(f\"- Train RMSE : {best_train_rmse:.4f}\\n\")\n",
    "    f.write(f\"- RMSE Gap   : {best_gap:.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Fare range error summary (Mean Absolute Error):\\n\")\n",
    "    f.write(fare_range_df.to_string(index=False))\n",
    "    f.write(\"\\n\\nPrediction correlations between models:\\n\")\n",
    "    f.write(corr_pred.to_string())\n",
    "    f.write(\"\\n\\nKey insights:\\n\")\n",
    "    f.write(\"- The best model is selected using lowest Test RMSE.\\n\")\n",
    "    f.write(\"- RMSE gap helps diagnose overfitting (large gap = overfit).\\n\")\n",
    "    f.write(\"- Fare-range errors show which model works better for low vs high fares.\\n\")\n",
    "    f.write(\"- Prediction correlations indicate how similarly the models behave.\\n\")\n",
    "    f.write(\"\\nRecommendations:\\n\")\n",
    "    f.write(\"- Use the best model above for deployment.\\n\")\n",
    "    f.write(\"- Consider further hyperparameter tuning on the best model.\\n\")\n",
    "    f.write(\"- Investigate high-residual trips (outliers) for data quality issues.\\n\")\n",
    "\n",
    "print(\"\\nSummary report saved to:\", report_path)\n",
    "print(\"\\n Model comparison & selection completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
